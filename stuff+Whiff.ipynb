{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Baseline Model - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Objectives``\n",
    "1. Implement a Baseline Models for run value prediction PRE BATTED-BALL\n",
    "2. Turn to a Random Forest for the another baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# decision tree\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# random forest|\n",
    "\n",
    "# misc\n",
    "import imblearn\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear output and stored data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('clear') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\">  Step 1: Data ingestion </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already created the training data in another file:\n",
    " <span style=\"color:gray\">TrackMan data of 2024 spring season</span> function below according to the following guidelines:\n",
    "\n",
    " a) Read all the csv files in the directory and merge them into a single dataframe \\\n",
    " b) Save the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need to run this again since already created the training data\n",
    "\n",
    "def load_data(path: str, num_columns=60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and merges CSV files from the specified directory, excluding files with 'player positioning' in their names.\n",
    "    \n",
    "    Parameters:\n",
    "    path (str): The directory path containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The merged DataFrame containing data from the selected CSV files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"The directory '{path}' does not exist.\")\n",
    "\n",
    "        # Get all files in the directory that end with .csv, excluding those with 'player positioning' in the name\n",
    "        all_files = [\n",
    "            file for file in glob.glob(f\"{path}/*.csv\") if 'player positioning' not in file\n",
    "        ]\n",
    "\n",
    "        # Raise an exception if no valid files are found\n",
    "        if not all_files:\n",
    "            raise ValueError(f\"No valid CSV files found in the directory '{path}'.\")\n",
    "\n",
    "        # Set the indices of the columns to keep\n",
    "        columns_to_keep = list(range(num_columns))  # will set that in the function call but usually 60 will be fine\n",
    "\n",
    "        # Read and merge the filtered files with the specified columns\n",
    "        df_list = [pd.read_csv(filename, usecols=columns_to_keep) for filename in all_files]\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Save the merged DataFrame to a CSV\n",
    "        output_path = \"/Users/tommayer/Desktop/games_test.csv\"\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"Error: {fnf_error}\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"Error: {val_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "I don't know if it's smart to load the data and concatenate all rows every time.  I could make it more like appending rows to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COLUMNS with data from pre-batted ball + RunsScored as our target variable\n",
    "required_columns = ['TaggedPitchType', 'AutoPitchType', 'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle', 'PitchCall',\n",
    "                      'SpinRate', 'SpinAxis', 'Tilt', 'Extension','InducedVertBreak', 'HorzBreak', 'VertApprAngle', 'HorzApprAngle']\n",
    "\n",
    "# RunsScored column is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/tommayer/Desktop/training_data.csv\"\n",
    "#data = load_data(path)\n",
    "data = pd.read_csv(path, usecols=required_columns)\n",
    "## drastically reduces the number of rows and columns -> way less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for Whiffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean column for Whiffs (True if PitchCall is 'StrikeSwinging', False otherwise)\n",
    "data['Whiff'] = data['PitchCall'] == 'StrikeSwinging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['PitchCall'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaggedPitchType</th>\n",
       "      <th>AutoPitchType</th>\n",
       "      <th>RelSpeed</th>\n",
       "      <th>VertRelAngle</th>\n",
       "      <th>HorzRelAngle</th>\n",
       "      <th>SpinRate</th>\n",
       "      <th>SpinAxis</th>\n",
       "      <th>Tilt</th>\n",
       "      <th>RelHeight</th>\n",
       "      <th>Extension</th>\n",
       "      <th>InducedVertBreak</th>\n",
       "      <th>HorzBreak</th>\n",
       "      <th>VertApprAngle</th>\n",
       "      <th>HorzApprAngle</th>\n",
       "      <th>Whiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>86.34831</td>\n",
       "      <td>-5.087035</td>\n",
       "      <td>-0.556059</td>\n",
       "      <td>2514.190308</td>\n",
       "      <td>69.694698</td>\n",
       "      <td>8:15</td>\n",
       "      <td>6.89596</td>\n",
       "      <td>5.20061</td>\n",
       "      <td>-1.26150</td>\n",
       "      <td>-6.71201</td>\n",
       "      <td>-12.231122</td>\n",
       "      <td>-1.751516</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastball</td>\n",
       "      <td>Four-Seam</td>\n",
       "      <td>94.49974</td>\n",
       "      <td>-3.133086</td>\n",
       "      <td>-0.492520</td>\n",
       "      <td>2095.787589</td>\n",
       "      <td>190.374426</td>\n",
       "      <td>12:15</td>\n",
       "      <td>6.87165</td>\n",
       "      <td>5.83655</td>\n",
       "      <td>20.20828</td>\n",
       "      <td>3.49654</td>\n",
       "      <td>-5.365324</td>\n",
       "      <td>0.134391</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastball</td>\n",
       "      <td>Four-Seam</td>\n",
       "      <td>94.81021</td>\n",
       "      <td>-3.910073</td>\n",
       "      <td>-1.135525</td>\n",
       "      <td>1996.806823</td>\n",
       "      <td>178.803234</td>\n",
       "      <td>12:00</td>\n",
       "      <td>6.94572</td>\n",
       "      <td>5.67326</td>\n",
       "      <td>22.06875</td>\n",
       "      <td>-0.43740</td>\n",
       "      <td>-5.815582</td>\n",
       "      <td>-1.213668</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>86.30865</td>\n",
       "      <td>-1.385858</td>\n",
       "      <td>-0.791508</td>\n",
       "      <td>3480.483920</td>\n",
       "      <td>100.930240</td>\n",
       "      <td>9:15</td>\n",
       "      <td>6.96039</td>\n",
       "      <td>5.43599</td>\n",
       "      <td>2.34610</td>\n",
       "      <td>-6.38485</td>\n",
       "      <td>-7.862841</td>\n",
       "      <td>-1.929632</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>87.45870</td>\n",
       "      <td>-4.605749</td>\n",
       "      <td>-1.323250</td>\n",
       "      <td>1287.761851</td>\n",
       "      <td>79.042003</td>\n",
       "      <td>8:45</td>\n",
       "      <td>6.99938</td>\n",
       "      <td>5.28786</td>\n",
       "      <td>0.27646</td>\n",
       "      <td>-4.37162</td>\n",
       "      <td>-11.264591</td>\n",
       "      <td>-2.102031</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TaggedPitchType AutoPitchType  RelSpeed  VertRelAngle  HorzRelAngle  \\\n",
       "0          Slider        Slider  86.34831     -5.087035     -0.556059   \n",
       "1        Fastball     Four-Seam  94.49974     -3.133086     -0.492520   \n",
       "2        Fastball     Four-Seam  94.81021     -3.910073     -1.135525   \n",
       "3          Slider        Slider  86.30865     -1.385858     -0.791508   \n",
       "4          Slider        Slider  87.45870     -4.605749     -1.323250   \n",
       "\n",
       "      SpinRate    SpinAxis   Tilt  RelHeight  Extension  InducedVertBreak  \\\n",
       "0  2514.190308   69.694698   8:15    6.89596    5.20061          -1.26150   \n",
       "1  2095.787589  190.374426  12:15    6.87165    5.83655          20.20828   \n",
       "2  1996.806823  178.803234  12:00    6.94572    5.67326          22.06875   \n",
       "3  3480.483920  100.930240   9:15    6.96039    5.43599           2.34610   \n",
       "4  1287.761851   79.042003   8:45    6.99938    5.28786           0.27646   \n",
       "\n",
       "   HorzBreak  VertApprAngle  HorzApprAngle  Whiff  \n",
       "0   -6.71201     -12.231122      -1.751516  False  \n",
       "1    3.49654      -5.365324       0.134391  False  \n",
       "2   -0.43740      -5.815582      -1.213668  False  \n",
       "3   -6.38485      -7.862841      -1.929632  False  \n",
       "4   -4.37162     -11.264591      -2.102031  False  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peer at data and get a sense of the shape\n",
    "data.head(5)\n",
    "#print(f'Data shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 2: Exploratory data analysis (EDA) </span>\n",
    "- check for missing values\n",
    "- check for duplicates\n",
    "- check for outliers\n",
    "- check for class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows to be dropped if N/A: \n",
    "- our target variables\n",
    "- name, date, location, team??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_required_columns = ['TaggedPitchType', 'AutoPitchType', 'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle',\n",
    "                      'SpinRate', 'SpinAxis', 'Tilt', 'Extension','InducedVertBreak', 'HorzBreak', 'VertApprAngle', 'HorzApprAngle']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows without certain columns\n",
    "data = data.dropna(subset=new_required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "# check how many rows were dropped\n",
    "print(f'Number of rows dropped: {data.shape[0] - len(data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedPitchType      object\n",
      "AutoPitchType        object\n",
      "RelSpeed            float64\n",
      "VertRelAngle        float64\n",
      "HorzRelAngle        float64\n",
      "SpinRate            float64\n",
      "SpinAxis            float64\n",
      "Tilt                 object\n",
      "RelHeight           float64\n",
      "Extension           float64\n",
      "InducedVertBreak    float64\n",
      "HorzBreak           float64\n",
      "VertApprAngle       float64\n",
      "HorzApprAngle       float64\n",
      "Whiff                  bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_vars = data.drop(['Whiff'], axis=1)\n",
    "dependent_var = data['Whiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_names = independent_vars.columns\n",
    "# reference this down below so i dont have to list them out there\n",
    "# Get column names by data type\n",
    "categorical_columns = independent_vars.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = independent_vars.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['TaggedPitchType', 'AutoPitchType', 'Tilt']\n",
      "Numerical columns: ['RelSpeed', 'VertRelAngle', 'HorzRelAngle', 'SpinRate', 'SpinAxis', 'RelHeight', 'Extension', 'InducedVertBreak', 'HorzBreak', 'VertApprAngle', 'HorzApprAngle']\n"
     ]
    }
   ],
   "source": [
    "print(f'Categorical columns: {categorical_columns}')\n",
    "print(f'Numerical columns: {numerical_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interaction terms between key features\n",
    "data['SpeedSpin'] = data['RelSpeed'] * data['SpinRate']\n",
    "data['BreakComposite'] = data['InducedVertBreak'] * data['HorzBreak']\n",
    "\n",
    "# Add polynomial features for important numerical variables\n",
    "data['RelSpeed_Squared'] = data['RelSpeed'] ** 2\n",
    "data['SpinRate_Squared'] = data['SpinRate'] ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction terms between top features\n",
    "data['VertAngle_Combined'] = data['VertRelAngle'] * data['VertApprAngle']\n",
    "data['HorzAngle_Combined'] = data['HorzRelAngle'] * data['HorzApprAngle']\n",
    "\n",
    "# Create velocity-based features\n",
    "data['SpeedSpin_Ratio'] = data['RelSpeed'] / data['SpinRate']\n",
    "data['PowerFeature'] = data['RelSpeed'] * data['RelSpeed']  # Squared velocity\n",
    "\n",
    "# Create break efficiency features\n",
    "data['TotalBreak'] = np.sqrt(data['InducedVertBreak']**2 + data['HorzBreak']**2)\n",
    "data['BreakRatio'] = data['InducedVertBreak'] / data['HorzBreak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 3: Data Preprocessing </span>\n",
    "- drop columns that are not useful?\n",
    "- encode labels \n",
    "- split into training and testing data\n",
    "- standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with certain data types: \\\n",
    "a) numerical data (float, int)  \\\n",
    "    - scale data \\\n",
    "    - RelSpeed, SpinRate, InducedVertBreak, HorzBreak, ExitSpeed, etc \\\n",
    "    \\\n",
    "b) categorical data (object/string) \\\n",
    "    - encode data (one-hot encoding with sklearn LabelEncoder) \\\n",
    "    - TaggedPitchType, AutoPitchType, PitchCall, KorBB, TaggedHitType, PlayResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocesses the data by identifying column types, encoding categorical data, and scaling numerical data.\n",
    "    Returns train/test/validation splits of features and target.\n",
    "\n",
    "    A series is a 1D array-like or list-like object that contains a single column of data (test and validation sets).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identify column types\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # 2. Handle categorical data\n",
    "    # For simple categorical variables, use Label Encoding\n",
    "    \"\"\" for col in ['TaggedPitchType', 'AutoPitchType']:\n",
    "        le = LabelEncoder()\n",
    "        data[f'{col}_encoded'] = le.fit_transform(data[col])\n",
    "        ## Note: label encoding assumes an order to the categories \"\"\"\n",
    "\n",
    "    # For nominal variables with many categories, use One-Hot Encoding\n",
    "    data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "    # 3. Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = numerical_columns\n",
    "    data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "    # 4. Split into features and target\n",
    "    X = data.drop(['Whiff'], axis=1)\n",
    "    y = data['Whiff']\n",
    "\n",
    "    # 6. Split into train/test/validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    \n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'X_val shape: {X_val.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "    print(f'y_val shape: {y_val.shape}')\n",
    "    print(f'y_test shape: {y_test.shape}')\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (673708, 91)\n",
      "X_val shape: (168428, 91)\n",
      "X_test shape: (210534, 91)\n",
      "y_train shape: (673708,)\n",
      "y_val shape: (168428,)\n",
      "y_test shape: (210534,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training the model, clean the data by replacing infinities and checking for very large values\n",
    "# Add this after your feature engineering steps and before model training\n",
    "\n",
    "# Replace infinities with NaN\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_val = X_val.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill NaN values with the median of each column\n",
    "for col in X_train.columns:\n",
    "    median_val = X_train[col].median()\n",
    "    X_train[col] = X_train[col].fillna(median_val)\n",
    "    X_val[col] = X_val[col].fillna(median_val)\n",
    "    X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# Clip very large values to a reasonable range\n",
    "# You might need to adjust these values based on your data\n",
    "max_val = 1e6\n",
    "min_val = -1e6\n",
    "X_train = X_train.clip(min_val, max_val)\n",
    "X_val = X_val.clip(min_val, max_val)\n",
    "X_test = X_test.clip(min_val, max_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part helps with class imbalance - assign weights to the classes based on their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Compute sample weights based on class frequencies\n",
    "sample_weights = compute_sample_weight('balanced', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 4: Modeling </span>\n",
    "- train a decision tree\n",
    "- train a random forest\n",
    "- train a gradient boosting machine (XGBoost)\n",
    "- compare the three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:\n",
    "- enseble method - combines multiple decision trees to make a prediction\n",
    "- uses bootstrap aggregating (bagging) - trains each tree on a different bootstrap sample of the data\n",
    "- uses random subspace method - trains each tree on a different random subset of the features\n",
    "- reduces variance and avoids overfitting\n",
    "- can handle both numerical and categorical data\n",
    "- easy to understand and interpret\n",
    "- prone to overfitting if not tuned properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "Mean Squared Error: 0.1103\n",
      "Root Mean Squared Error: 0.3321\n",
      "R² Score: -0.2185\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "               feature  importance\n",
      "9        VertApprAngle    0.124384\n",
      "5            RelHeight    0.095058\n",
      "1         VertRelAngle    0.088928\n",
      "16  HorzAngle_Combined    0.061034\n",
      "15  VertAngle_Combined    0.057973\n",
      "6            Extension    0.057007\n",
      "2         HorzRelAngle    0.056116\n",
      "10       HorzApprAngle    0.044439\n",
      "19          TotalBreak    0.040620\n",
      "17     SpeedSpin_Ratio    0.039886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommayer/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the correct model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the model\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=20,        # Maximum depth of trees (None means unlimited)\n",
    "    min_samples_split=2,   # Minimum samples required to split\n",
    "    random_state=1,       # For reproducibility\n",
    "    n_jobs=-1        # Use all CPU cores\n",
    ")\n",
    "model_rf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Make predictions\n",
    "predictions_rf = model_rf.predict(X_val)\n",
    "\n",
    "# Evaluate the model (using regression metrics instead of accuracy)\n",
    "mse = mean_squared_error(y_val, predictions_rf)\n",
    "rmse = mean_squared_error(y_val, predictions_rf, squared=False)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_val, predictions_rf)\n",
    "\n",
    "print(f'Random Forest Performance:')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "# Feature importance (optional)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model_rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "- max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a smaller parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    cv=3,  # Reduce the number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation:\n",
    "- use cross-validation to evaluate the model's performance on the training data\n",
    "- use the validation set to tune the hyperparameters\n",
    "- use the test set to evaluate the final model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation RMSE: 0.2674 (+/- 0.0004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model_rf, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"Cross-validation RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation RMSE: 0.2674 (+/- 0.0004)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Performance:\n",
      "Mean Squared Error: 0.0845\n",
      "Root Mean Squared Error: 0.2906\n",
      "R² Score: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommayer/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the model\n",
    "model_xgb = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=9,\n",
    "    random_state=42\n",
    ") # define the model\n",
    "\n",
    "model_xgb.fit(X_train, y_train) # train the model\n",
    "\n",
    "# Make predictions\n",
    "predictions_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, predictions_xgb)\n",
    "rmse = mean_squared_error(y_val, predictions_xgb, squared=False)\n",
    "r2 = r2_score(y_val, predictions_xgb)\n",
    "\n",
    "print(f'XGBoost Performance:')\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommayer/.pyenv/versions/3.12.6/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "colsample_bytree: 0.815750979360025\n",
      "learning_rate: 0.21518913081944233\n",
      "max_depth: 9\n",
      "min_child_weight: 2\n",
      "n_estimators: 290\n",
      "subsample: 0.9468795734220015\n",
      "\n",
      "Best RMSE: 0.2720\n",
      "\n",
      "Best Model Performance:\n",
      "Root Mean Squared Error: 0.2677\n",
      "R² Score: 0.2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommayer/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter space\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'min_child_weight': randint(1, 7)\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"\\nBest RMSE: {(-random_search.best_score_)**0.5:.4f}\")\n",
    "\n",
    "# Create model with best parameters\n",
    "best_model_xgb = XGBRegressor(**random_search.best_params_, random_state=42)\n",
    "best_model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate best model\n",
    "best_predictions = best_model_xgb.predict(X_val)\n",
    "best_rmse = mean_squared_error(y_val, best_predictions, squared=False)\n",
    "best_r2 = r2_score(y_val, best_predictions)\n",
    "\n",
    "print(f'\\nBest Model Performance:')\n",
    "print(f'Root Mean Squared Error: {best_rmse:.4f}')\n",
    "print(f'R² Score: {best_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found:\n",
    "colsample_bytree: 0.815750979360025\n",
    "learning_rate: 0.21518913081944233\n",
    "max_depth: 9\n",
    "min_child_weight: 2\n",
    "n_estimators: 290\n",
    "subsample: 0.9468795734220015\n",
    "\n",
    "Best RMSE: 0.2720\n",
    "\n",
    "Best Model Performance:\n",
    "Root Mean Squared Error: 0.2677\n",
    "R² Score: 0.2080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Performance:\n",
    "Mean Squared Error: 0.0845\n",
    "Root Mean Squared Error: 0.2906\n",
    "R² Score: 0.0667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "# Create stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom scorer that weights recall more heavily\n",
    "scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    model_rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=skf,\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Expectancy Model from Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_run_expectancy_data(data):\n",
    "    \"\"\"\n",
    "    Prepare run expectancy data with proper game/inning boundaries\n",
    "    \"\"\"\n",
    "    # Assuming you have or can add these columns:\n",
    "    required_columns = [\n",
    "        'GameID',        # Unique identifier for each game\n",
    "        'InningID',      # Inning number\n",
    "        'InningHalf',    # Top/Bottom\n",
    "        'RunsScored'\n",
    "    ]\n",
    "    \n",
    "    # Sort data chronologically\n",
    "    data = data.sort_values(['GameID', 'InningID', 'InningHalf'])\n",
    "    \n",
    "    # Calculate runs scored for rest of inning\n",
    "    def calculate_future_runs(group):\n",
    "        # Sum runs scored after each pitch until end of inning\n",
    "        group['future_runs'] = group['RunsScored'].iloc[::-1].cumsum().iloc[::-1] - group['RunsScored']\n",
    "        return group\n",
    "    \n",
    "    # Group by game and inning to respect boundaries\n",
    "    data = data.groupby(['GameID', 'InningID', 'InningHalf']).apply(calculate_future_runs)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_run_expectancy_model(data):\n",
    "    \"\"\"\n",
    "    Train model to predict runs scored in remainder of inning\n",
    "    \"\"\"\n",
    "    # Features that could predict run expectancy\n",
    "    features = [\n",
    "        'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle',\n",
    "        'SpinRate', 'SpinAxis', 'InducedVertBreak', 'HorzBreak',\n",
    "        'VertApprAngle', 'HorzApprAngle',\n",
    "        # Add dummy variables for pitch types\n",
    "        *[col for col in data.columns if col.startswith('TaggedPitchType_')]\n",
    "    ]\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data['future_runs']  # Target is runs scored in remainder of inning\n",
    "    \n",
    "    # Split respecting game boundaries\n",
    "    game_ids = data['GameID'].unique()\n",
    "    train_games, test_games = train_test_split(game_ids, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X[data['GameID'].isin(train_games)]\n",
    "    X_test = X[data['GameID'].isin(test_games)]\n",
    "    y_train = y[data['GameID'].isin(train_games)]\n",
    "    y_test = y[data['GameID'].isin(test_games)]\n",
    "    \n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "def calculate_pitch_run_values(data, model):\n",
    "    \"\"\"\n",
    "    Calculate run value for each pitch\n",
    "    \"\"\"\n",
    "    # Group by game and inning\n",
    "    def process_inning(group):\n",
    "        # Get features for prediction\n",
    "        features = [\n",
    "            'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle',\n",
    "            'SpinRate', 'SpinAxis', 'InducedVertBreak', 'HorzBreak',\n",
    "            'VertApprAngle', 'HorzApprAngle',\n",
    "            *[col for col in group.columns if col.startswith('TaggedPitchType_')]\n",
    "        ]\n",
    "        \n",
    "        # Calculate run expectancy before and after each pitch\n",
    "        re_before = model.predict(group[features])\n",
    "        re_after = np.roll(re_before, -1)  # Shift predictions up by one\n",
    "        re_after[-1] = 0  # Last pitch in inning has 0 future run expectancy\n",
    "        \n",
    "        # Calculate run value\n",
    "        run_value = re_before - re_after + group['RunsScored']\n",
    "        \n",
    "        return run_value\n",
    "    \n",
    "    data['run_value'] = data.groupby(['GameID', 'InningID', 'InningHalf']).apply(process_inning)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Usage example:\n",
    "def analyze_run_values():\n",
    "    # Prepare data\n",
    "    data = prepare_run_expectancy_data(data)\n",
    "    \n",
    "    # Train model\n",
    "    model, X_test, y_test = train_run_expectancy_model(data)\n",
    "    \n",
    "    # Calculate run values\n",
    "    data = calculate_pitch_run_values(data, model)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nAverage Run Values by Pitch Type:\")\n",
    "    print(data.groupby('TaggedPitchType')['run_value'].mean().sort_values(ascending=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='TaggedPitchType', y='run_value', data=data)\n",
    "    plt.title('Run Values by Pitch Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
