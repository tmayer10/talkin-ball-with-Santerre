{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Baseline Model - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``Objectives``\n",
    "1. Implement a Baseline Models for run value prediction PRE BATTED-BALL\n",
    "2. Turn to a Random Forest for the another baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# decision tree\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# random forest|\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear output and stored data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('clear') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\">  Step 1: Data ingestion </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already created the training data in another file:\n",
    " <span style=\"color:gray\">TrackMan data of 2024 spring season</span> function below according to the following guidelines:\n",
    "\n",
    " a) Read all the csv files in the directory and merge them into a single dataframe \\\n",
    " b) Save the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need to run this again since already created the training data\n",
    "\n",
    "def load_data(path: str, num_columns=60) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and merges CSV files from the specified directory, excluding files with 'player positioning' in their names.\n",
    "    \n",
    "    Parameters:\n",
    "    path (str): The directory path containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The merged DataFrame containing data from the selected CSV files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"The directory '{path}' does not exist.\")\n",
    "\n",
    "        # Get all files in the directory that end with .csv, excluding those with 'player positioning' in the name\n",
    "        all_files = [\n",
    "            file for file in glob.glob(f\"{path}/*.csv\") if 'player positioning' not in file\n",
    "        ]\n",
    "\n",
    "        # Raise an exception if no valid files are found\n",
    "        if not all_files:\n",
    "            raise ValueError(f\"No valid CSV files found in the directory '{path}'.\")\n",
    "\n",
    "        # Set the indices of the columns to keep\n",
    "        columns_to_keep = list(range(num_columns))  # will set that in the function call but usually 60 will be fine\n",
    "\n",
    "        # Read and merge the filtered files with the specified columns\n",
    "        df_list = [pd.read_csv(filename, usecols=columns_to_keep) for filename in all_files]\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        # Save the merged DataFrame to a CSV\n",
    "        output_path = \"/Users/tommayer/Desktop/games_test.csv\"\n",
    "        merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    except FileNotFoundError as fnf_error:\n",
    "        print(f\"Error: {fnf_error}\")\n",
    "    except ValueError as val_error:\n",
    "        print(f\"Error: {val_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "I don't know if it's smart to load the data and concatenate all rows every time.  I could make it more like appending rows to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY COLUMNS with data from pre-batted ball + RunsScored as our target variable\n",
    "required_columns = ['TaggedPitchType', 'AutoPitchType', 'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle', 'PitchCall',\n",
    "                      'SpinRate', 'SpinAxis', 'Tilt', 'Extension','InducedVertBreak', 'HorzBreak', 'VertApprAngle', 'HorzApprAngle']\n",
    "\n",
    "# RunsScored column is gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/tommayer/Desktop/training_data.csv\"\n",
    "#data = load_data(path)\n",
    "data = pd.read_csv(path, usecols=required_columns)\n",
    "## drastically reduces the number of rows and columns -> way less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new column for Whiffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean column for Whiffs (True if PitchCall is 'StrikeSwinging', False otherwise)\n",
    "data['Whiff'] = data['PitchCall'] == 'StrikeSwinging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['PitchCall'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TaggedPitchType</th>\n",
       "      <th>AutoPitchType</th>\n",
       "      <th>RelSpeed</th>\n",
       "      <th>VertRelAngle</th>\n",
       "      <th>HorzRelAngle</th>\n",
       "      <th>SpinRate</th>\n",
       "      <th>SpinAxis</th>\n",
       "      <th>Tilt</th>\n",
       "      <th>RelHeight</th>\n",
       "      <th>Extension</th>\n",
       "      <th>InducedVertBreak</th>\n",
       "      <th>HorzBreak</th>\n",
       "      <th>VertApprAngle</th>\n",
       "      <th>HorzApprAngle</th>\n",
       "      <th>Whiff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>86.34831</td>\n",
       "      <td>-5.087035</td>\n",
       "      <td>-0.556059</td>\n",
       "      <td>2514.190308</td>\n",
       "      <td>69.694698</td>\n",
       "      <td>8:15</td>\n",
       "      <td>6.89596</td>\n",
       "      <td>5.20061</td>\n",
       "      <td>-1.26150</td>\n",
       "      <td>-6.71201</td>\n",
       "      <td>-12.231122</td>\n",
       "      <td>-1.751516</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastball</td>\n",
       "      <td>Four-Seam</td>\n",
       "      <td>94.49974</td>\n",
       "      <td>-3.133086</td>\n",
       "      <td>-0.492520</td>\n",
       "      <td>2095.787589</td>\n",
       "      <td>190.374426</td>\n",
       "      <td>12:15</td>\n",
       "      <td>6.87165</td>\n",
       "      <td>5.83655</td>\n",
       "      <td>20.20828</td>\n",
       "      <td>3.49654</td>\n",
       "      <td>-5.365324</td>\n",
       "      <td>0.134391</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastball</td>\n",
       "      <td>Four-Seam</td>\n",
       "      <td>94.81021</td>\n",
       "      <td>-3.910073</td>\n",
       "      <td>-1.135525</td>\n",
       "      <td>1996.806823</td>\n",
       "      <td>178.803234</td>\n",
       "      <td>12:00</td>\n",
       "      <td>6.94572</td>\n",
       "      <td>5.67326</td>\n",
       "      <td>22.06875</td>\n",
       "      <td>-0.43740</td>\n",
       "      <td>-5.815582</td>\n",
       "      <td>-1.213668</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>86.30865</td>\n",
       "      <td>-1.385858</td>\n",
       "      <td>-0.791508</td>\n",
       "      <td>3480.483920</td>\n",
       "      <td>100.930240</td>\n",
       "      <td>9:15</td>\n",
       "      <td>6.96039</td>\n",
       "      <td>5.43599</td>\n",
       "      <td>2.34610</td>\n",
       "      <td>-6.38485</td>\n",
       "      <td>-7.862841</td>\n",
       "      <td>-1.929632</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slider</td>\n",
       "      <td>Slider</td>\n",
       "      <td>87.45870</td>\n",
       "      <td>-4.605749</td>\n",
       "      <td>-1.323250</td>\n",
       "      <td>1287.761851</td>\n",
       "      <td>79.042003</td>\n",
       "      <td>8:45</td>\n",
       "      <td>6.99938</td>\n",
       "      <td>5.28786</td>\n",
       "      <td>0.27646</td>\n",
       "      <td>-4.37162</td>\n",
       "      <td>-11.264591</td>\n",
       "      <td>-2.102031</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TaggedPitchType AutoPitchType  RelSpeed  VertRelAngle  HorzRelAngle  \\\n",
       "0          Slider        Slider  86.34831     -5.087035     -0.556059   \n",
       "1        Fastball     Four-Seam  94.49974     -3.133086     -0.492520   \n",
       "2        Fastball     Four-Seam  94.81021     -3.910073     -1.135525   \n",
       "3          Slider        Slider  86.30865     -1.385858     -0.791508   \n",
       "4          Slider        Slider  87.45870     -4.605749     -1.323250   \n",
       "\n",
       "      SpinRate    SpinAxis   Tilt  RelHeight  Extension  InducedVertBreak  \\\n",
       "0  2514.190308   69.694698   8:15    6.89596    5.20061          -1.26150   \n",
       "1  2095.787589  190.374426  12:15    6.87165    5.83655          20.20828   \n",
       "2  1996.806823  178.803234  12:00    6.94572    5.67326          22.06875   \n",
       "3  3480.483920  100.930240   9:15    6.96039    5.43599           2.34610   \n",
       "4  1287.761851   79.042003   8:45    6.99938    5.28786           0.27646   \n",
       "\n",
       "   HorzBreak  VertApprAngle  HorzApprAngle  Whiff  \n",
       "0   -6.71201     -12.231122      -1.751516  False  \n",
       "1    3.49654      -5.365324       0.134391  False  \n",
       "2   -0.43740      -5.815582      -1.213668  False  \n",
       "3   -6.38485      -7.862841      -1.929632  False  \n",
       "4   -4.37162     -11.264591      -2.102031  False  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peer at data and get a sense of the shape\n",
    "data.head(5)\n",
    "#print(f'Data shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 2: Exploratory data analysis (EDA) </span>\n",
    "- check for missing values\n",
    "- check for duplicates\n",
    "- check for outliers\n",
    "- check for class imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows to be dropped if N/A: \n",
    "- our target variables\n",
    "- name, date, location, team??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['PitchCall']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3n/cwnbs7r54tg9sdh6j221x5z80000gn/T/ipykernel_18299/3867934873.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# drop rows without certain columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6666\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ['PitchCall']"
     ]
    }
   ],
   "source": [
    "# drop rows without certain columns\n",
    "data = data.dropna(subset=required_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 0\n"
     ]
    }
   ],
   "source": [
    "# check how many rows were dropped\n",
    "print(f'Number of rows dropped: {data.shape[0] - len(data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedPitchType      object\n",
      "AutoPitchType        object\n",
      "PitchCall            object\n",
      "RelSpeed            float64\n",
      "VertRelAngle        float64\n",
      "HorzRelAngle        float64\n",
      "SpinRate            float64\n",
      "SpinAxis            float64\n",
      "Tilt                 object\n",
      "RelHeight           float64\n",
      "Extension           float64\n",
      "InducedVertBreak    float64\n",
      "HorzBreak           float64\n",
      "VertApprAngle       float64\n",
      "HorzApprAngle       float64\n",
      "Whiff                  bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_vars = data.drop(['Whiff'], axis=1)\n",
    "dependent_var = data['Whiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_names = independent_vars.columns\n",
    "# reference this down below so i dont have to list them out there\n",
    "# Get column names by data type\n",
    "categorical_columns = independent_vars.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = independent_vars.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['TaggedPitchType', 'AutoPitchType', 'PitchCall', 'Tilt']\n",
      "Numerical columns: ['RelSpeed', 'VertRelAngle', 'HorzRelAngle', 'SpinRate', 'SpinAxis', 'RelHeight', 'Extension', 'InducedVertBreak', 'HorzBreak', 'VertApprAngle', 'HorzApprAngle']\n"
     ]
    }
   ],
   "source": [
    "print(f'Categorical columns: {categorical_columns}')\n",
    "print(f'Numerical columns: {numerical_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Add interaction terms between key features\\ndata['SpeedSpin'] = data['RelSpeed'] * data['SpinRate']\\ndata['BreakComposite'] = data['InducedVertBreak'] * data['HorzBreak']\\n\\n# Add polynomial features for important numerical variables\\ndata['RelSpeed_Squared'] = data['RelSpeed'] ** 2\\ndata['SpinRate_Squared'] = data['SpinRate'] ** 2 \""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Add interaction terms between key features\n",
    "data['SpeedSpin'] = data['RelSpeed'] * data['SpinRate']\n",
    "data['BreakComposite'] = data['InducedVertBreak'] * data['HorzBreak']\n",
    "\n",
    "# Add polynomial features for important numerical variables\n",
    "data['RelSpeed_Squared'] = data['RelSpeed'] ** 2\n",
    "data['SpinRate_Squared'] = data['SpinRate'] ** 2 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PitchCall'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPitchCall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['PitchCall'] not found in axis\""
     ]
    }
   ],
   "source": [
    "data = data.drop(columns=['PitchCall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 3: Data Preprocessing </span>\n",
    "- drop columns that are not useful?\n",
    "- encode labels \n",
    "- split into training and testing data\n",
    "- standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with certain data types: \\\n",
    "a) numerical data (float, int)  \\\n",
    "    - scale data \\\n",
    "    - RelSpeed, SpinRate, InducedVertBreak, HorzBreak, ExitSpeed, etc \\\n",
    "    \\\n",
    "b) categorical data (object/string) \\\n",
    "    - encode data (one-hot encoding with sklearn LabelEncoder) \\\n",
    "    - TaggedPitchType, AutoPitchType, PitchCall, KorBB, TaggedHitType, PlayResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocesses the data by identifying column types, encoding categorical data, and scaling numerical data.\n",
    "    Returns train/test/validation splits of features and target.\n",
    "\n",
    "    A series is a 1D array-like or list-like object that contains a single column of data (test and validation sets).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Identify column types\n",
    "    numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # 2. Handle categorical data\n",
    "    # For simple categorical variables, use Label Encoding\n",
    "    \"\"\" for col in ['TaggedPitchType', 'AutoPitchType']:\n",
    "        le = LabelEncoder()\n",
    "        data[f'{col}_encoded'] = le.fit_transform(data[col])\n",
    "        ## Note: label encoding assumes an order to the categories \"\"\"\n",
    "\n",
    "    # For nominal variables with many categories, use One-Hot Encoding\n",
    "    data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "    # 3. Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = numerical_columns\n",
    "    data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "    # 4. Split into features and target\n",
    "    X = data.drop(['Whiff'], axis=1)\n",
    "    y = data['Whiff']\n",
    "\n",
    "    # 6. Split into train/test/validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    \n",
    "\n",
    "    print(f'X_train shape: {X_train.shape}')\n",
    "    print(f'X_val shape: {X_val.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}')\n",
    "    print(f'y_train shape: {y_train.shape}')\n",
    "    print(f'y_val shape: {y_val.shape}')\n",
    "    print(f'y_test shape: {y_test.shape}')\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PitchCall'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, X_val, y_train, y_test, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[61], line 21\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" for col in ['TaggedPitchType', 'AutoPitchType']:\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    le = LabelEncoder()\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    data[f'{col}_encoded'] = le.fit_transform(data[col])\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    ## Note: label encoding assumes an order to the categories \"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# For nominal variables with many categories, use One-Hot Encoding\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 3. Scale numerical features\u001b[39;00m\n\u001b[1;32m     24\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/reshape/encoding.py:169\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.6/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['PitchCall'] not in index\""
     ]
    }
   ],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" from sklearn.utils.class_weight import compute_class_weight\\n\\nclass_weights = compute_class_weight('balanced', \\n                                    classes=np.unique(y_train), \\n                                    y=y_train)\\nclass_weight_dict = dict(zip(np.unique(y_train), class_weights)) \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                    classes=np.unique(y_train), \n",
    "                                    y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part helps with class imbalance - assign weights to the classes based on their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Compute sample weights based on class frequencies\n",
    "sample_weights = compute_sample_weight('balanced', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### <span style=\"color:chocolate\"> Step 4: Modeling </span>\n",
    "- train a decision tree\n",
    "- train a random forest\n",
    "- train a gradient boosting machine (XGBoost)\n",
    "- compare the three models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:\n",
    "- enseble method - combines multiple decision trees to make a prediction\n",
    "- uses bootstrap aggregating (bagging) - trains each tree on a different bootstrap sample of the data\n",
    "- uses random subspace method - trains each tree on a different random subset of the features\n",
    "- reduces variance and avoids overfitting\n",
    "- can handle both numerical and categorical data\n",
    "- easy to understand and interpret\n",
    "- prone to overfitting if not tuned properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "Mean Squared Error: 0.0000\n",
      "Root Mean Squared Error: 0.0000\n",
      "R² Score: 1.0000\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                     feature  importance\n",
      "42  PitchCall_StrikeSwinging         1.0\n",
      "0                   RelSpeed         0.0\n",
      "58                 Tilt_1:15         0.0\n",
      "67                 Tilt_3:30         0.0\n",
      "66                 Tilt_3:15         0.0\n",
      "65                 Tilt_3:00         0.0\n",
      "64                 Tilt_2:45         0.0\n",
      "63                 Tilt_2:30         0.0\n",
      "62                 Tilt_2:15         0.0\n",
      "61                 Tilt_2:00         0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommayer/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the correct model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the model\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees\n",
    "    max_depth=20,        # Maximum depth of trees (None means unlimited)\n",
    "    min_samples_split=2,   # Minimum samples required to split\n",
    "    random_state=1,       # For reproducibility\n",
    "    n_jobs=-1        # Use all CPU cores\n",
    ")\n",
    "model_rf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Make predictions\n",
    "predictions_rf = model_rf.predict(X_val)\n",
    "\n",
    "# Evaluate the model (using regression metrics instead of accuracy)\n",
    "mse = mean_squared_error(y_val, predictions_rf)\n",
    "rmse = mean_squared_error(y_val, predictions_rf, squared=False)  # Root Mean Squared Error\n",
    "r2 = r2_score(y_val, predictions_rf)\n",
    "\n",
    "print(f'Random Forest Performance:')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "# Feature importance (optional)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model_rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-Parameter Tuning:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- min_samples_leaf\n",
    "- max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a smaller parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV instead of GridSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    cv=3,  # Reduce the number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation:\n",
    "- use cross-validation to evaluate the model's performance on the training data\n",
    "- use the validation set to tune the hyperparameters\n",
    "- use the test set to evaluate the final model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation RMSE: 0.2573 (+/- 0.0059)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model_rf, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "print(f\"Cross-validation RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the model\n",
    "model_xgb = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions_xgb = model_xgb.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, predictions_xgb)\n",
    "rmse = mean_squared_error(y_val, predictions_xgb, squared=False)\n",
    "r2 = r2_score(y_val, predictions_xgb)\n",
    "\n",
    "print(f'XGBoost Performance:')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Expectancy Model from Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_run_expectancy_data(data):\n",
    "    \"\"\"\n",
    "    Prepare run expectancy data with proper game/inning boundaries\n",
    "    \"\"\"\n",
    "    # Assuming you have or can add these columns:\n",
    "    required_columns = [\n",
    "        'GameID',        # Unique identifier for each game\n",
    "        'InningID',      # Inning number\n",
    "        'InningHalf',    # Top/Bottom\n",
    "        'RunsScored'\n",
    "    ]\n",
    "    \n",
    "    # Sort data chronologically\n",
    "    data = data.sort_values(['GameID', 'InningID', 'InningHalf'])\n",
    "    \n",
    "    # Calculate runs scored for rest of inning\n",
    "    def calculate_future_runs(group):\n",
    "        # Sum runs scored after each pitch until end of inning\n",
    "        group['future_runs'] = group['RunsScored'].iloc[::-1].cumsum().iloc[::-1] - group['RunsScored']\n",
    "        return group\n",
    "    \n",
    "    # Group by game and inning to respect boundaries\n",
    "    data = data.groupby(['GameID', 'InningID', 'InningHalf']).apply(calculate_future_runs)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_run_expectancy_model(data):\n",
    "    \"\"\"\n",
    "    Train model to predict runs scored in remainder of inning\n",
    "    \"\"\"\n",
    "    # Features that could predict run expectancy\n",
    "    features = [\n",
    "        'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle',\n",
    "        'SpinRate', 'SpinAxis', 'InducedVertBreak', 'HorzBreak',\n",
    "        'VertApprAngle', 'HorzApprAngle',\n",
    "        # Add dummy variables for pitch types\n",
    "        *[col for col in data.columns if col.startswith('TaggedPitchType_')]\n",
    "    ]\n",
    "    \n",
    "    X = data[features]\n",
    "    y = data['future_runs']  # Target is runs scored in remainder of inning\n",
    "    \n",
    "    # Split respecting game boundaries\n",
    "    game_ids = data['GameID'].unique()\n",
    "    train_games, test_games = train_test_split(game_ids, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X[data['GameID'].isin(train_games)]\n",
    "    X_test = X[data['GameID'].isin(test_games)]\n",
    "    y_train = y[data['GameID'].isin(train_games)]\n",
    "    y_test = y[data['GameID'].isin(test_games)]\n",
    "    \n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model, X_test, y_test\n",
    "\n",
    "def calculate_pitch_run_values(data, model):\n",
    "    \"\"\"\n",
    "    Calculate run value for each pitch\n",
    "    \"\"\"\n",
    "    # Group by game and inning\n",
    "    def process_inning(group):\n",
    "        # Get features for prediction\n",
    "        features = [\n",
    "            'RelSpeed', 'RelHeight', 'VertRelAngle', 'HorzRelAngle',\n",
    "            'SpinRate', 'SpinAxis', 'InducedVertBreak', 'HorzBreak',\n",
    "            'VertApprAngle', 'HorzApprAngle',\n",
    "            *[col for col in group.columns if col.startswith('TaggedPitchType_')]\n",
    "        ]\n",
    "        \n",
    "        # Calculate run expectancy before and after each pitch\n",
    "        re_before = model.predict(group[features])\n",
    "        re_after = np.roll(re_before, -1)  # Shift predictions up by one\n",
    "        re_after[-1] = 0  # Last pitch in inning has 0 future run expectancy\n",
    "        \n",
    "        # Calculate run value\n",
    "        run_value = re_before - re_after + group['RunsScored']\n",
    "        \n",
    "        return run_value\n",
    "    \n",
    "    data['run_value'] = data.groupby(['GameID', 'InningID', 'InningHalf']).apply(process_inning)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Usage example:\n",
    "def analyze_run_values():\n",
    "    # Prepare data\n",
    "    data = prepare_run_expectancy_data(data)\n",
    "    \n",
    "    # Train model\n",
    "    model, X_test, y_test = train_run_expectancy_model(data)\n",
    "    \n",
    "    # Calculate run values\n",
    "    data = calculate_pitch_run_values(data, model)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nAverage Run Values by Pitch Type:\")\n",
    "    print(data.groupby('TaggedPitchType')['run_value'].mean().sort_values(ascending=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='TaggedPitchType', y='run_value', data=data)\n",
    "    plt.title('Run Values by Pitch Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
